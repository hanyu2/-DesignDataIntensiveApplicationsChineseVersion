# 第三章

# 存储与检索

> 建立秩序，省却搜索
>
> ——德国谚语

在最底层的层面上，数据库需要做两件事：当你给他一些数据，他应该存储这些数据，当你后来想要这些数据，他应该返回给你。

在第二章中，我们他评论了数据模型和查询语言——比如说，你（开发者）给数据库的数据的格式，和后来你想要数据的机制。在本章中我们也会从数据库的角度来讨论同样的事：我们如何存储给的数据，我们在索取时如何查找？

作为一个程序开发者，你为什么要在乎数据库内部是如何存储和检索的呢？你大概了不会从零自己写一个自己的存储引擎，但是你需要为你的应用程序在很多选项中选择一个合适的存储引擎？为了调节你的存储引擎针对你的程序的负载表现更好，你应该存储引擎底层的工作原理有一个大致的了解。

尤其是，有些引擎是针对事务优化的，有些只针对分析优化的，这两个之间有着巨大的差别。我们会在后文的“事务处理还是分析”中探索他们之间的不同，在“列式存储”中我们会讨论针对分析的一些存储引擎。

但是，首先我们通过你熟悉的一些数据库的存储引擎（传统关系型数据库和大部分所谓的NoSQL数据库）来开始本章。我们会审视存储引擎家族：*日志结构*存储引擎，和*面向页面*的存储引擎，比如B树。

## 驱动数据库的数据结构

考虑一下世界上最简单的数据库，用两个Bash函数来实现：

```bash
#!/bin/bash
db_set () {
    echo "$1,$2" >> database
}

db_get () {
    grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

这两个函数实现了一个KV存储。执行 `db_set key value` ，会将 键（key）和值（value）存储在数据库中。键和值（几乎）可以是任何东西——比如，值可以是JSON文档。然后当你执行db_get key，它会查找跟那个键关联的最新值并返回。

运行起来像这样：

```bash
$ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}' $ 

$ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'

$ db_get 42
{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
```

底层的存储格式特别简单：是一个文本文档，里面每一行都有一个键值对，用都好分隔开（有点像一个CSV文件，不考虑转义问题）。每次调用`db_set`会向文档末尾追加，旧值不会被重写——你需要在文档中找一个键的最后一次出现的时候，并找到最新的值（因此 `db_get` 中使用了 `tail -n 1`）：

```bash
$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}' 

$ db_get 42
{"name":"San Francisco","attractions":["Exploratorium"]}

$ cat database
123456,{"name":"London","attractions":["Big Ben","London Eye"]}
42,{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
42,{"name":"San Francisco","attractions":["Exploratorium"]}
```

`dn_set`函数在一些很简单的操作是有着非常好的性能，因为向文件中追加是很高效的。就像`db_set`函数做的那样，很多数据库底层用了*日志*，这是一种只追加的数据文件。现实的数据库有更多事情要处理（比如并发控制，回收磁盘空间以避免日志无限增长，处理错误与部分写入的记录），但是基本的原理是一样的。日志是不可思议得有用，我们在本书剩下的内容里会多次遇到他们。

> *日志*通常代指应用程序日志，应用程序会输出文档来描述发生了什么。在这本书中，*日志*有一个更广泛的意义：一个只可追加的记录序列。它不需要是人类可读的，有可能是二进制或者为某种特定的程序读取。

在另一方面，我们的`db_get`函数在当数据库记录特别多的时候，性能会特别差。每次你想找一个键的时候，`db_get`函数需要从头到尾扫描整个数据库，找到那个键出现的地方。用算法符号来描述就是，查找的代价是`O(n)`：如果你的数据库记录数量`n`翻倍，查询时以前的两倍。这样不太好。

为了高效在数据库找到一个键对应的值，我们需要一种不同的数据结构：*索引*。在本章中，我们会看一些索引结构，并比较他们；他们背后的基本思想就是额外维护一些元数据，用在当做路标帮助你确定数据的位置。如果你想用急种不同的方法搜索相同的数据，你可能需要不同部分数据上的不同索引来做到。

索引是一种从主数据中*额外*的衍生出来的结构。一些数据库允许你添加和删除索引，但这样不会影响到你数据库的数据；他只影响查询的性能。维护额外的结构会带来损耗，尤其在写入操作上。对于写入，很难击败单纯的项文件追加的性能，因为这是写操作的最简单的可行性了。任何种类的索引同堂都会拖慢写入速度，因为每次写数据时要更新索引。

早数据存储系统中有一种很重要的权衡：尽心选的的索引加速查询速度，但是每个索引会拖慢写入速度。为此，数据库默认不会为每个东西建索引，但是要求你——应用程序开发这或者数据库管理员——来根据你程序的典型查询模式来人工选择索引。你需要选择能带给你数据库最好性能的索引，在不带来不必要的更多的性能损耗的前提下。

### 哈希索引

让我们从KV数据的索引开始。这不是唯一的一种你可以建索引的数据，但事实非常常见的，对于建立更加复杂索引的模块是很有用的。

KV存储跟大部分编程语言中的*字典*是非常相似的，通常使用哈希表来实现的。哈希表在很多算法教科书中都有描述。所以在这里我们不会探讨他们工作的细节。我们已经在我们内存中的数据结构里有哈希表了，为什么不为磁盘上数据建索引呢？

如果我们的数据存储中需要向文件中追加，就像前面的例子那样。做简单可行的策略是：在内存中维护一个哈希表每个键映射到数据文件中的一个字节偏移量——指向了你要找的值的位置，就像图3-1中展示的那样。当你在文件中追加一个KV对的时候，你需要在哈希表中更新来映射你写入的数据的偏移量（这样对插入新键和更新现有键都可用）。当你想找一个值的时候，用哈希表找到数据文件中的偏移量，找到那个位置，读出那个值。

![](https://vonng.gitbooks.io/ddia-cn/content/img/fig3-1.png)

*图3-1 在类CSV格式的文件中存储KV对的日志，用内存上的哈希表做索引*

这个方法听起来简单，但是是一种可信的方法。实际上，Bitcask实际上就是这么做的（Riak中默认的存储引擎）。Bitcask提供一个高效地读写，但所有键必须能放入可用内存中，因为哈希表是完全需要在内存中维护的。值可以比可用内存更大，因为他们通过一次磁盘查找就能从磁盘中加载好。如果数据文件中的一部分已经在文件系统的缓存中加载好，读取就不需要任何磁盘IO。

类似于Bitcaskde存储引擎适合一个键的值经常更新的情况。比如，键可能是一个猫咪视频的URL，值节能是视频的播放次数（用户每次点击播放按钮时都会+1）。在这种负载下，有很多写入，但是不会有很多不同的键——每个键有很多的写操作，但是将所有键保存在内存中是可行的。

直到现在，我们只是追加写入一个文件——我们如何避免最终磁盘空间不足呢？一个好的解决办法是把日志分成固定大小的段，在写入的数据到固定大小关闭段文件，让接下来的写入写到新的段文件中。我们可以在这些段文件上进行压缩（compaction），就像图3-2中展示的那样。压缩意味着丢掉日志中的重复的键，维护每个键的最新的更新。

![](https://vonng.gitbooks.io/ddia-cn/content/img/fig3-2.png)

*图3-2 KV更新日志（计算每个猫咪视频的播放次数）的压缩，对每个键只保留最新的值*

除此之外，因为压缩通常会把每个段文件变得更小（假设一个段文件中的键平均被写入很多次），我们可以在执行压缩的时候把很多段文件合并在一起，就像图3-3中展示的那样。段文件再被写入后就不再会被更改，所以合并的段文件会被写入到一个新文件中。冻结后的段文件的合并和压缩介意在后台线程中进行，所以在执行合并和压缩的时候会用旧的段文件，我们仍然可以继续像平常一样提供读写的请求。当合并进程完成的时候，我们会把读取请求从旧的段文件切换到新的合并后的段文件上——然后旧的段文件可以容易地删掉。

![](https://vonng.gitbooks.io/ddia-cn/content/img/fig3-3.png)

*图3-3 同时在段上执行压缩和合并操作*

现在每个段都有自己的在内存上的哈希秒，每个键映射到文件的偏移量。为了找到一个键的值，我们先检查最新的段的哈希表：如果键不存在，找的第二最新的段，以此类推。合并过程把段的控制得很小，所以查询不需要检查太多的哈希表。

把这个简单的思想应用在实践中有许多细节需要考虑。简要来说，在实际的实践中有一些重要的问题：

- 文件格式

CSV不是最好的日志格式。用二进制格式首先编码字符串的长度，然后跟着字符串（不需要转义），这样会更快也会更简单。

- 删除记录

如果你想删除一个键和他的值，你需要在你要删除的数据文件后面追加一个特殊的删除记录（通常称作墓碑）。当日志段被合并的时候，墓碑告诉合并程序来丢弃删除的键的所有旧值。

- 崩溃恢复

如果数据库重启了，那么内存里的哈希表就丢掉了。原则上来说，你可以通过从头到尾读取整个段文件，一个个来标注每个键对应的最新值来恢复段的哈希表。但是如果段文件很大的话，这可能会消耗很长的时间，这样重启过程会很长。Bitcask通过在磁盘上存储每个段文件哈希表的快照（加载到内存更快）来加速恢复过程。

- 不完全的写记录

数据库任何时间都有可能宕机，包括在向日志追加记录的半路上。Bitcasks的文件包括校验和，这样可以发现并忽略日志中的损坏部分。

- 并发控制

当追加日志时有严格的顺序要求，一个常用的实现方法是只保存一个写线程。数据文件的段是只可追加的，或者是不可变得，所以他们能通过对多线程来并发读取。

一个只可追加的日志乍看起来有点浪费：你为什么不在文件中更新，用新的值来覆盖旧的值。但因为几个原因，只可追加的设计有这些好处：

- 追加和段合并是有序的写操作，这样比随机写通常更快，尤其是在磁盘旋转硬盘上。在某种程度上在SSD上也是更受欢迎的。我们会在接下来的“比较B-树和LSM-树”中进一步讨论这个问题。
- 如果段文件是只可追加或者不可变的情况下，并发和崩溃恢复快的更多。比如，你不需要担心在写一个值的时候突然崩溃，这样会留给你一个混合了旧值和新值的的文件。

但是，哈希表索引也有不足：

- 哈希表必须放在内存里，如果你有很多很多键，你就不太幸运了。原则上老说，你可以在磁盘上维护一个哈希表，但不幸的很难让磁盘上的哈希表高效运行。那需要很多随机访问IO，当它变满时增长是很昂贵的，并且散列冲突需要很多的逻辑。
- 范围查询效率不高。比如你不能很轻易地扫描在kitty00000和kitty99999之间的所有键——你需要哈希表里的每一个键。

在下一节中，我们会看到一些没有这些限制的索引结构。

## SSTables和LSM树

在图3-3中，每个日志结构的存储段都是一个KV对的序列。这些对按照写入的顺序出现，后来的日志中的值优先于较早日志中的值。除此之外，文件中键值对的顺序并没影响。

现在我么可以对我们的段文件的格式做一个简答的修改：我们需要KV对的顺序按照K顺序排列。乍一看，这个要求让我们没法继续顺序写入，但是我们一会再讨论。

我们把这种格式叫做*排序字符串表*或者简称*SSTable*。我们同样要求每个合并的段文件中的键只能出现一次（压缩过程已经保证）。SSTable跟哈希索引的日志段有一些很强的有点：

1. 合并段既简单又高效，甚至当文件大于可用内存是也可以。这种方法像是*合并排序*的算法（图3-4）一样，你并排读取输入文件，在每个文件中找到第一个键，把最小的键(根据排序顺序)复制到输出文件中。这个过程会产生一个新的合并段文件，也叫按键排序。

![](https://vonng.gitbooks.io/ddia-cn/content/img/fig3-4.png)

*图3-4 合并几个SSTable段，对于每个键只保留最新的值*

如果相同的键出现在几个输入段中咋整？记住每个段包含了一段时间中所有值写到数据库的值。这意味着一个输入的段文件中的所有值比其他段文件中的所有值都更新（假设我们总是合并相邻的段）。当多个段都包含相同的键，我们可以留下最新的段中的值，丢掉旧段中的值

2. 为了在文件中找到一个特定的键，你不需要再对内存中的所有键维护一个索引。拿图3-5来做例子：如果你正在找`handiwork`这个键，但你不知道这个键在段文件中确切的偏移量。但是你知道`handbag`和`handsome`的偏移量，因为是有序的，所以你知道`handiwork`肯定在这两个之间出现。这就意味着你可以跳到`handbag`，从哪里开始扫描直到你找到`handiwork`（如果键不在这个文件中没法找到）

![](https://vonng.gitbooks.io/ddia-cn/content/img/fig3-4.png)

*图3-5一个有内存索引的SSTable*

你仍然需要一个内存中的索引来告诉你一些键的偏移量，但这可以是很稀疏的：每几千字节的段文件就有一个键就足够了，因为几千字节可以很快被扫描。

> 如果所有的键和值都有固定大小，你可以在段文件上用二分搜索，来避免完全使用内存索引。但是实践中键值通常都是变长的，因此如果没有索引，就很难知道记录的分界点（前一条记录结束，后一条记录开始的地方）

3. 因为读取请求无论如何都需要扫描一个区间内的一些KV对，就有可能把这些记录聚合到一个块中，并在写到磁盘（图3-5的阴影部分）之前压缩。每个稀疏的内存中的的索引每个条目都指向一个压缩块的起始点。除了节省磁盘空间，压缩也降低了IO的带宽使用。

### 构造和维护SSTables

现在看起来都是很好的——但是首先你要如何让你的数据按键排序？我们的写入可以是任何顺序。

维护一个磁盘上的有序的结构是可行的（参考“B树”），但是在内存里维护简单得多。你可以用有很多有名的树结构，比如红黑树或者AVL树。在这些数据结构的帮助下，你可以把键按照任何顺序插入，然后有序地读取。

我们现在的存储引擎工作起来像这样：

- 写入的时候，把它加到内存里的平衡的树结构（比如红黑树）。这个内存中的树有时叫做*memtable*
- 当memtable变得越来越大超过一个阈值（通常是几MB）的时候，将其作为SSTable文件写入磁盘。这个过程可以高效完成因为树已经按照键的顺序排序了KV对。这个新的SSTable文件变成了数据库里最新的段，当SSTable写到磁盘上的时候，可以写到一个新的memtable实例。
- 为了服务读请求，首先试着在memtable里找那个键，然后在最新的磁盘上的段里找，然后在次新的段中找，以此类推
- 有时候会在后台做合并和压缩的线程来合并段文件和丢弃重写的或者删除的值

这种模式工作起来还不错，只有一个问题：如果数据库挂了，最新的写入（已经在memtable但是还没写到磁盘上）的就丢失了。为了避免这种问题，我们可以在磁盘上维护一个另外的日志，每个写入都会立即被附加到磁盘上，就像前一节中的一样。这个日志不用是排序好的，但是并不重要，因为他唯一的目的是在memtable宕机后恢复。每次memtable写出到一个SSTable的时候，对应的日志文件就可以丢掉。

### 从SSTables做出一个LSM-Tree

这里描述的算法本质上就是LevelDB和RocksDB，KV存储引擎库用到的，他们设计成嵌套到其他的应用程序里。在其他的方面，LevelDB可以在Riak中用作Bitcask的替代品。相同的存储引擎也被用在Cassandra和HBase里，这俩都是受Google的Bigtable论文（引入了SSTable和memtable这俩术语）启发。

最初这种索引结构是由Patrick O'Neil等人以*日志结构合并树*（或LSM树）描述的，在之前的日至结构的文件系统基础上构建的。基于这种合并和压缩有序文件原理的存储引擎通常被叫做LSM存储引擎。

Lucene是一种被用在全文搜索Elasticsearch和Solar上的索引引擎，他用了一种相似的方法来存储*字典*。一个全文所以比一个KV索引复杂得多，但是给予的是相似的思想：在一个查询中给定一个词，找到所有提到这个词的文档（网页，产品描述等）。这用一种KV结构来实现，K是一个词，V是所有包含这个词的文档ID列表。在Lucene里，这种从词到文档ID列表的映射被存在类似于SSTable的有序文件中，有必要时这个也会在后台合并。

### 性能优化

和往常一样，实际中要让存储引擎高效工作也有很多细节需要考虑。比如，LSM树的算法在查询一个数据库中不存在的键是可能很慢，你得先查memtable，再一路查到最老的段文件（可能每个都要从磁盘读取），这样才能确定键是不存在的。为了优化这种访问，存储引擎经常使用额外的*布隆过滤器*（布隆过滤器是一中内存高效的数据结构，用来近似判断集合内容，他可以告诉你一个键在数据库是否存在，省去了查询不存在的键做的不必要的磁盘读取）。

有很多不同的策略来确定SSTables压缩和合并的顺序和时间。最常见的选择是*大小分级*和*分层压缩*。LevelDB和RocksDB用分层压缩（所以才有了LevelDB这名），HBase用大小分级，Cassandra同时支持两种。在大小分级的压缩中，更新更小的SSTables会一直合并到更旧更大的SSTables里，在分层压缩里，键的区间被分成到更小的SSTables，旧的数据被移动到了另外的“层”，这样可以让压缩递增地进行并使用更少的磁盘空间。

即使有许多微妙的东西，但是LSM树的基本思想 —— 保存一些在后台合并的SSTables的层次 —— 简单而有效。甚至在数据集比可用内存更大的时候也能继续正常工作。因为数据是排好序存储的，你可以高效地执行区间查询（扫描从某个最小值到最大值的所有键），因为磁盘的写入是有序的，LSM树可以支持非常高的写入吞吐量。

## B树

我们目前讨论的日志结构的索引正在逐渐被接受，但是他们不是最常见的索引。最广泛应用的索引结构是非常不同的：B树。

B树在1970年被引入，不到10年后变得“无处不在”，B树经受了时间的考验。他成为了几乎所有关系数据库的标准的索引实现，很多非关系数据库也用他们。

像SSTables一样，B树按照键的顺序维护KV对，这样保证高效地KV查找和区间查询。但相同点到此为止：B树采用了一种非常不同的设计思想。

我们之前看到的日志结构索引把数据库分成了可变长度的段，通常是几MB或者更大，并且总是按顺序编写段。不同的是，B树把数据库分成固定大小的*块*或者*页*，通常是4KB（有时候更大），每次读或者写一个页。这种设计更接近于底层硬件，因为磁盘通常都是组织在固定大小的块中。

每个页可以用一个地址或者位置来确定，这样能让一个页指向另一个页——类似于指针，但是是在磁盘上而不是在内存上。我们这一种这些也引用来构造一个页的树，就像图3-6展示的那样。

![](https://vonng.gitbooks.io/ddia-cn/content/img/fig3-6.png)

*图3-6 用B树索引查找一个键*

一个页被指定成B树的*根节点*，从那里开始在索引里查一个键。一个页包含了一些键和指向子页的引用。每个子节点负责一个连续区间的键，引用之间的键指示了区间的边界。

在图3-6的例子里，我们查找键251，所以我们知道我们要顺着边界在200到300之间的页引用。这将我们带到一个类似的页，进一步打破了200 - 300到子范围。

最终我们到了一个含有单个键（叶子页面）的页，这个页内存了每个键的值或者存着值得其他页面引用。

B树里一个页指向子页面的引用数量叫做*分支因子*，比如图3-6的分支因子是6。十几种，分支因子取决于存储页引用和区间便捷的空间大小，通常是几百。

如果你想在B树里更新一个已有的键的值，你要找到包含那个键的叶子页面，在页面里更改值，再把页写回到磁盘（指向那个页的引用仍然有效）。如果你想增加一个新的键，你需要找到区间包含那个键的页，把键加到那个页里。如果页里没有足够的空间来容纳新的键，就把他分成两个半满的页，然后更新父页来包含新的键区间的分区，参考图3-7

> 往B树差一个新键是很直接的，但是删除一个键（要维护树平衡）要牵扯很多其他的东西

![](https://vonng.gitbooks.io/ddia-cn/content/img/fig3-7.png)

*图3-7 通过分解一个页来让B树增长*

这个算法保证了树还是*平衡的*：有n个键的B树总是有O(log n)的深度。大多数数据库都能放一个三到四层的树，所以你不需要顺着很多引用来找到你需要的页。（页大小为4KB，分支因子是500的四层的树可以存最多256TB）。

### 让B树更可靠

B树的基本的底层写入操作是在磁盘上永新数据写重写一个页。假设重写不会改变页的位置，在一个页被重写是所有到这个页的引用都不会被改变。这跟日志结构索引（比如LSM树，只向文件追加，最终删除过时文件）是完全不同的，他从不修改文件。

你可以把在磁盘上重写一个页想象成一个实际的硬件操作。在一个磁性硬盘上，这意味着把硬盘指针移动到正确位置，等待磁盘转到正确位置，然后永新数据写到正确的地方。在SSD上发生的就更复杂一点了，因为SSD每次必须一次擦除和重写相当大的存储芯片块。

除此之外，有些操作需要写几个不同的页。比如，如果因为写满导致的分页，你需要向分开的两个页面写，也要重写他们的父页来更新指向这两个子页的引用。这是一个危险的操作，因为如果在只写了一部分的页的时候数据库宕机，你的索引就是损坏的（比如，有可能有一个*孤儿*页面没有任何父页）。

为了让数据库对宕机更有韧性，通常在B树的视线里包括一个磁盘上的额外的数据结构：*预写式日志*（WAL, write-ahead-log，也称为重做日志*redo log*）。每次修改B树的时候都要在向树的页面里写之前都要先写入这个只可追加的文件。当数据库从宕机中重启时，这个日志用来把B树恢复到一个一致的状态。

更新页面的一个额外的复杂情况是，在多线程同时访问B数的时候要很谨慎地控制并发访问——否则一个线程会看到树的不一致的状态。这个通常是用*latches锁存器*（轻量锁）来保护树的数据结构来实现。日志结构化的方法在这方面更简单，因为它们在后台进行所有的合并，而不会干扰传入的查询，并且一直地将旧的段原子交换为新的段。

### B树优化

因为B树存在了这么久，这些年有很多优化也并不奇怪。举几个例子：

- 不用重写页和维护WAL来做当即恢复，有些数据库（比如LMDB）用一种写时复制方案。被修改的页被写到一个不一样的地方，树里会新建一个新的父页指到这个新的地方。这种方式发对并发控制也有用，我们将在“快照隔离和可重复读”中看到。
- 我们可以通过在页里不存整个键来省空间，我们可以简写键。尤其是对树的内部的页上。键只需要提供足够的信息来当做键区间之间的边界就可以。把更多的键放在一个页中让树具有更高的分支因子，因此更少的层。
- 总体来说，页面可以放到磁盘上的任何地方：没有要求让相邻的键曲边放在磁盘上相邻的位置上。如果一个哈讯需要按顺序扫描键区间的一个很大的部分，那么每个页面的布局可能会非常不方便，因为每个读取的页面都可能需要磁盘查找。因此，许多B树实现尝试分开布局树，使得叶子页面按顺序出现在磁盘上。但是，当树增长是很难维护这种顺序。相反的是LSM树在合并的时候一次性重写大的存储的段，所以它们更容易使顺序键在磁盘上彼此靠近。
- 额外的指针也加到了树里。比如，每个叶子页面可以有他的左右兄弟页面的引用，这样可以顺序扫描键而不用跳回到父页面。
- B树的变体例如*分形树*，借用了一些日志结构的思想来减少磁盘查找（而且它们与分形并无关系）

## 比较B树和LSM树

即便B树的实现普遍比LSM树的实现更成熟，LSM树因为他们的新能特点也是很有意思的。根据经验，LSM树通常写入更快，而B树读取更快。读取通常在LSM树向慢一点因为他们在不同的压缩阶段需要检查几种不同的数据结构和SSTables。

但是，跑分经常是不能包含所有情况，对负载的细节也很敏感。你需要用特定的负载来测试你的系统，这样才能得到有效的对比。在这一届中，我们会简要地讨论在测量存储引擎性能时值得讨论的东西。

### LSM树的优点

一个B树要写两份数据：第一次写入预先日志，一次写到树本身（有可能当页分解时要再写一次）。即使在一个页的几字节改变的时候，也有要一次重写整个页的开销。有些存储引擎甚至会重写同一个页面两次，为的是避免由于宕机导致的页面的部分更新。

由于重复的压缩和合并SSTables，日志结构的索引也需要些好几次。这种影响——在数据库的生命周期中写入数据库导致对磁盘的多次写入 ——这种被称为*写入放大*。需要特别注意的是固态硬盘，固态硬盘的闪存寿命在重写有限次数后就会耗尽。

在写入繁重的应用程序中，性能瓶颈可能是数据库写到磁盘上的速度。在这种情况下，读写放大对性能成本有直接影响：存储引擎往磁盘写得越多，在可用的磁盘带宽内每秒处理的写入就越少。

除此之外，LSM树通常可以比B树承受更高的写入吞吐量，一部分原因为他们有时候有着较低的读写放大（尽管这受存储引擎的配置和负载的影响），一部分是因为他们顺序写压缩的SSTtable文件而不是在树里重写一些页。这种区别对磁性硬盘尤其重要，因为磁性硬盘上顺序写入比随机写入快得多。

LSM树可以更好地被压缩，因此在磁盘上比B树会产生更小的文件。B树的存储引擎由于碎片化会在磁盘留下很多未使用的空间：当一个页被分开或者当已有的一个页没法容下一行，页里的一部分空间就没有被使用。因为LSM树并不是针对页的，会周期性的重写SSTables来避免碎片，所以他们有着更小的存储开销，尤其是在使用层级压缩的时候。

在很多SSD上，固件内部使用日志结构化算法来把随机写入转成顺序写入底层存储芯片上，因此存储引擎写入模式的影响不太明显。但是更低的写入放大和更少的碎片仍是SSD的优势：更紧凑地组织数据能能够在有限的IO带宽下允许更多的读写请求。

### LSM树的劣势

日志结构存储的一个劣势就是压缩过程有时候会影响正在读取和写入的性能。即使存储引擎尝试递增压缩而不影响并发访问，磁盘有有限的资源，所以很容易发生一个请求需要等待磁盘完成一个非常耗时的压缩操作。这个对吞吐量和平均响应时间的影响通常是很小的，但是在更高百分比的情况下。日志结构存储引擎查询的响应时间有时候会特别高，但B树是更可预料的。

压缩过程在高写入吞吐量时的问题是：磁盘的有限写入带宽需要在初始写入（记录和刷新内存表到磁盘）和在后台运行的压缩线程之间共享。当写入一个空数据库是，全部的的磁盘带宽可以用来做初始写入，当时当数据库越来越大的时候，越来越多的磁盘带宽要用来做压缩。

如果写入吞吐量很高而且压缩没有仔细配置，有可能发生压缩速度赶不上写入速度。在这种情况下，磁盘上未合并的段的数量会一直增长直到你的磁盘耗光，读取也会变慢因为你要检查更多的段文件。通常来说，基于SSTables的存储引擎不会限制写入速度，即便压缩速度跟不上的情况下，所以你需要仔细监控来发现这种情况。

B树的一个优势，键是在索引中只存在一个地方，但是日志结构的存储引擎在多个段文件里有多个相同键的副本。这样让支持强事务的数据库倾向B树：在很多关系数据库中，事务隔离的实现会把一个区间内的的键加锁，在B树索引中，这些键可以直接连接到树上。在第七章中我们会讨论这个点的更多细节。

B树在数据库架构和为很多负载提供一致性的高性能里都是根深蒂固的。在新数据库中，日志结构的索引正在变得越来越受欢迎。没有多快好省的方法来帮助你为你的事情场景选择正确类型的存储引擎，所以值得进行一些经验上的测试。

## 其他索引结构

至今，我们已经讨论了KV索引，这个像是关系数据库里的*主键*索引。主键在关系数据库里唯一标识一行，在文档数据库是标识一个文档，在图数据库里标识一个顶点。数据库里的其他记录的主键可以引用那一行/文件/顶点，索引就是用来解决这种索引的。

*二级索引*也很常见。在关系数据库中，你在同一个表上可以创建几个二级索引，用`CREATE INDEX`这个命令，他们常对执行高效的join至关重要要。比如，在第二章的图2-1中，你很有可能想在`user_id`上建立二级索引，所以你就能在每个表中属于同一个用户的所有行。

可以在KV索引上很容易地建立二级索引。主要的不同是键不是唯一的，比如，有很多行（文档，顶点）有相同的键。有两种方法可以解决：或者通过使索引中的每个值，成为匹配行标识符的列表（如全文索引中的发布列表），或者通过向每个索引添加行标识符来使每个关键字唯一。无论哪种方式，B树和日志结构索引都可以用作辅助索引。

### 在索引里存储值

索引里的键就是查询想要找的东西，但是值可能是两个东西：可以是要查询的真实行（文档，顶点），也可以是存在另外地方的这个行的引用。在后者中，行存储的地方叫做*堆文件*，他存数据的时候没有特定的顺序（有可能是只可追加，有可能会记录删除的行来用新的数据重写）。堆文件的方法是很常见的，因为他在多个二级索引存在的时候避免数据重复：每个索引只是引用了堆文件里的一个位置，实际的数据存在另一个地方。

在更新一个值而不改变键的时候，堆文件的方法会是非常高效的：只要新值不比旧值更大，记录旧可以原地重写。如果新值更大，这种情况就更加复杂，因为有可能需要在堆里移动到一个新的有更大空间地方。在那种情况下，要么所有的索引都需要更新，以指向记录的新堆位置，或者在旧堆位置留下一个转发指针。

在某些情况下，从索引调到堆文件的额外的跳跃对读性能影响太大，所以更想把被索引的行直接存在一个索引里。这被称作*聚集索引*。比如，MySQL的二级索引指向主键（而不是堆文件的位置）。在SQL Server中，你可以每个表指明一个聚集索引。

一种聚集索引（把所有的行数据存在索引里）和非聚集索引（值在索引里存指向数据的引用）折衷的方法被称为*覆盖索引（covering index）*或*包含列的索引（index with included columns）*，这种会在索引里存*一些*表的列。这样允许一些查询通过单独用索引就可以完成（在这种情况下，索引称为*覆盖*了查询）。

和任何类型的数据重复一样，聚集和覆盖索引都可以加速读取速度，但是要求额外的存储，也会增加写的开销。数据库也要浪费额外的力气来保证事务，因为应用程序会因为数据重复看到不一致的情况。

### 多列索引

至今为止讨论的索引都是从一个键映射到一个值。如果我们要同时查一个表里的多个列是不足够的（或者文档里的多个字段）。

最常见的多列索引的类型叫做*拼接索引*，简单地通过把以列追加到另一列（索引定义了用什么顺序来拼接字段）来把几个字段结合成一个键来完成。这个有点像老式的电话本一样，提供了从（姓，名）到电话的索引。因为排序，索引可以用来找一个特定姓的所有人，或者一个特定姓和名结合的所有人。但是，你想找有一个名的所有人索引就没用了。

多维索引是一次查询多个列的一个泛用的方法，对地理空间数据尤为重要。比如，一个餐厅搜索的网站要在数据库里有每个餐厅的经纬度。当用户在地图上搜索餐厅的时候，网站要在用户正在浏览的长方形地图内查询所有的餐厅。这需要二维的区间查询：

```sql
SELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079 
                           AND longitude > -0.1162 AND longitude < -0.1004;
```

一个标准的B树或者LSM树索引不足以有效满足这些查询：他给你的是一个经度（没纬度）区间内或者纬度（没经度）区间内（可能是南北极之间的任何地方）的所有餐厅，但是不能同时查询。

一种选项是把二维的位置用空间填充曲线转化成一个数字，用常用的B树来索引。更常见的方法是使用特殊化的空间索引，例如R树。比如，PostGIS使用PostgreSQL的通用Gist工具将地理空间索引实现为R树。这里我们没有足够的地方来描述R树，但是有大量的文献可供参考。

一个有意思的想法是多维索引不仅只用作地理位置。比如，一个电商网站你可以在维度（红，绿，蓝）上使用三维索引来搜索一种颜色区间内的商品，或者在一个气象观测的数据库中在（日期，气温）上使用二维索引来高效搜索2013年气温在25到30°C之间的观测记录。用一维的索引，你可以幺妹扫描所有2013年的记录（没有气温）然后用气温来过滤结果，要么用相反的方法。一个二维所用可以同时通过时间戳和气温来缩小范围。这个技术被HyperDex使用。

### 全文搜索和模糊索引

至今讨论的所有索引都假设你有准确的数据，允许你用查询一个键的准确值，或者用查询一个键一些排好序的值。不允许你做的是搜索*相似*键，不如拼写错误的单词。这种*模糊*查询需要不同的技术。

比如，全文搜索引擎通常允许查询一个词来查询其扩展的同义词，这样来忽略一个词语法上的各种变形，也用来查询一变形词在相同文档中的出现，支持各种文本语义分析的功能。为了处理文档中或者查询中的拼写错误，Lucene可以在文本中查询在一个特定编辑距离（编辑距离1意思是一个字母被增，删或者改）中的词。

在“从SSTables中构造LSM树”中提到的，Lucene使用了一种类似SSTable的野狗作为术语字典。这种结构需要一个小的内存中的索引来告诉查询在排序文件中哪个偏移量需要查找关键字。在LevelDB中，这种内存中的索引是一些键的的稀疏集合，类似于*字典树*。这个自动机可以转换成*Levenshtein*自动机，它支持在给定的编辑距离内有效地搜索单词。

其他的模糊搜索技术正朝着文档分类和机器学习的方向发展。有关更多详细信息，请参阅信息检索教科书。

### 把所有东西存在内存中

至今在本章讨论的数据结构都是对磁盘限制的回答。与主内存相比，磁盘处理起来很尴尬。对于磁性硬盘和SSD，如果要在读取和写入时获得良好性能，则需要仔细地布置磁盘上的数据。但是，我们可以容忍这种尴尬因为磁盘有两个很大的优点：它们是耐用的（它们的内容在电源关闭时不会丢失），并且每GB的成本比RAM低。

RAM变得越来越便宜，每GB的成本在下降。很多数据集没那么大，所以把他们完全放在内存里是非常可行的，有可能是分布在很多机器上。这样带来了*内存数据库*的开发。

有一些内存的KV数据库，比如Memcached，仅用于缓存，在重新启动计算机时丢失的数据是可以接受的。但是其他的内存数据库的目标是持久性，可以用特殊的硬件（比如电池供电的RAM），通过在磁盘上写修改的日志，或者通过定时快照写入磁盘，或者通过把内存状态复制到其他机器上来达到。

当内存数据库重启的时候，需要重载他的状态，从磁盘或者通过网络从副本（除非用了特殊的硬件）中重载。尽管写到了磁盘上，但他仍然是内存数据库，因为磁盘只是为了持久性作为只可追加的日志，读取是完全内存服务的。写到磁盘也有操作优势：磁盘上的文件可以用外部工具轻松备份，检查和分析。

像是VoltDB，MemSQL和Oracle TimesTen这样的产品是关系模型的内存数据库，数据库厂商生产他们可以通过移除所有的管理盘上的数据结构的开销来极大提升性能。RAMCloud 是一个开源的，内存的KV可持久（用日志结构的方法来处理内存和磁盘上的数据）的数据库。Redis和Couchbase通过异步写磁盘来提供弱持久性。

反直觉的是，内存数据库的性能优势并不是由于他们不需要从磁盘读取。即使你有足够的磁盘空间，存储引擎也可能从来不需要从磁盘读取，因为操作系统会在内存里缓存最近使用的磁盘块。而且，这样可能更快，因为他们可以避免把内存中的数据编码成用来在磁盘上格式的开销。

除了性能，另外一个与意思的内存数据库的方面是能提供基于磁盘的索引很难实现的数据模型。比如，Redis给很多数据结构（像是priority queue和set）提供一种类似数据库的接口，这种实现是相对简单的。

最新的研究表明一个内存数据库架构可以扩展支持比内存还大的数据集，不会带来以磁盘为中心的架构的开销。这种所谓的*反缓存*的方法会在内存不足时从内存上把最近最少使用的数据写到磁盘上，然后以后需要再次访问的再加载回内存。这类似于操作系统处理虚拟内存和交换文件，但是数据库可以比操作系统更高效地管理内存，数据库也可以在单条记录的粒度上工作而不是整个内存页面。这种方式仍然需要索引来完全放入内存（类似本章开始Bitcask的例子）。

如果*非易失性存储器（NVM)*的技术想要更广泛地被采纳，就需要更多的对存储引擎设计的改变。现在这是一个新的研究领域，但是值得未来的关注。









